{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marie Nguyen \n",
    "\n",
    "#### LAB 03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading packages\n",
    "import math\n",
    "import string\n",
    "import xml.etree.ElementTree as ET\n",
    "# Sources: https://docs.python.org/3/library/xml.etree.elementtree.html \n",
    "\n",
    "from collections import OrderedDict\n",
    "# Resource: https://www.geeksforgeeks.org/python-sort-python-dictionaries-by-key-or-value/ \n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 1: Implement a tf.idf retrieval algorithm in your code, which uses the weighted sum formula with tf.idf weighting (above) as the similarity function. Set k=2. \n",
    "\n",
    "a) Tokenize exactly as you did in the previous assignment.\n",
    "\n",
    "b) Your inverted lists, in addition to docid, will also need to store tf for each document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a linked list \n",
    "# Source: https://www.geeksforgeeks.org/python-linked-list/ and Data Structure course\n",
    "\n",
    "# Initialize a Node class to create a node \n",
    "class Node:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.next = None\n",
    "\n",
    "# Initialize a Linkedlist class\n",
    "class LinkedList:\n",
    "    def __init__(self):\n",
    "        self.head = None\n",
    "\n",
    "    # Add a node to the Linkedlist\n",
    "    def add(self, data):\n",
    "        # Initialize a new node with the data \n",
    "        new_node = Node(data)\n",
    "        # If the Linkedlist is empty, assign the new node is the head \n",
    "        if not self.head:\n",
    "            self.head = new_node\n",
    "            return\n",
    "        # Otherwise, let the current node is the head \n",
    "        current = self.head\n",
    "        # Loop through the end of the Linkedlist \n",
    "        while current.next != None:\n",
    "            current = current.next\n",
    "        # Add a new node to the end. \n",
    "        current.next = new_node\n",
    "    \n",
    "    # Calculate the length of a Linkedlist\n",
    "    def length(self):\n",
    "        # Assign current node as the head of the Linkedlist\n",
    "        current = self.head\n",
    "        count = 0  # Initilize a variable for counting\n",
    "        # Interate the the Linkedlist\n",
    "        while (current != None):\n",
    "            # Increment the count and move to the next node\n",
    "            count += 1\n",
    "            current = current.next\n",
    "        return count\n",
    "                  \n",
    "    def toString(self):\n",
    "        # Assign a current node as the head \n",
    "        current = self.head\n",
    "        # Initialize an empty string for printing out the linkedlist\n",
    "        content = \" \"\n",
    "        # Loop through all nodes in the linked list\n",
    "        while (current != None):\n",
    "            # Add the data of the node to the content \n",
    "            content += \"    \" + str(current.data) + \" \\n \"\n",
    "            # Move to the next node \n",
    "            current = current.next\n",
    "            \n",
    "        # content += \"None\"\n",
    "        return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define function for calculating term frequency, given a XML file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tf_dictionary(xml_file, stopping, stemming):\n",
    "    \n",
    "    # Initialize an array containing stop words:\n",
    "    with open(\"stoplist\", \"r\") as stoplistFile:\n",
    "        stopwords_array = stoplistFile.read().lower().split() #Tokenization\n",
    "        \n",
    "    # Initialize a dictionary for the term frequency \n",
    "    term_freq_dict = {}\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    tree = ET.parse(xml_file) \n",
    "    root = tree.getroot()\n",
    "    \n",
    "    \"\"\" Parsing only the headline and text sections of each document\n",
    "        and save them in a new document \"\"\"\n",
    "        \n",
    "    # Loop through each <DOC> element\n",
    "    for doc in root.findall(\"DOC\"):\n",
    "        terms_array = []\n",
    "        \n",
    "        # Find the headline and text elements\n",
    "        headline = doc.find(\"HEADLINE\")\n",
    "        text_doc = doc.find(\"TEXT\")\n",
    "        # Find document number element \n",
    "        doc_number = doc.find(\"DOCNO\").text\n",
    "        \n",
    "        # Remove punctuation and apply lower case\n",
    "        punctTable = str.maketrans(string.punctuation, \" \" * len(string.punctuation))\n",
    "        headline = headline.text.lower().translate(punctTable)\n",
    "        text_doc = text_doc.text.lower().translate(punctTable)\n",
    "            \n",
    "        # Get unique terms from headline in each document \n",
    "        for term in headline.split():\n",
    "            if term not in terms_array:\n",
    "                # On stemming \n",
    "                if stopping == True: \n",
    "                    if term not in stopwords_array:\n",
    "                        terms_array.append(term)\n",
    "                # On stemming      \n",
    "                elif stemming == True:\n",
    "                    terms_array.append(stemmer.stem(term)) #Nomalization\n",
    "                # Original \n",
    "                else:\n",
    "                    terms_array.append(term)\n",
    "        \n",
    "        # Get unique terms from text in each document \n",
    "        for term in text_doc.split():\n",
    "            if term not in terms_array:\n",
    "                # On stopping \n",
    "                if stopping == True: \n",
    "                    if term not in stopwords_array:\n",
    "                        terms_array.append(term)\n",
    "                # On stemming        \n",
    "                elif stemming == True:\n",
    "                    term = stemmer.stem(term) #Nomalization\n",
    "                    terms_array.append(stemmer.stem(term)) #Nomalization\n",
    "                # Original     \n",
    "                else:\n",
    "                    terms_array.append(term)\n",
    "        \n",
    "        # Count each term frequency in the each document\n",
    "        tf_subdict = {}\n",
    "        \n",
    "        # Iterate through the terms array \n",
    "        for term in terms_array:\n",
    "            # If it is a unique term, tf = 1; Otherwise, update tf \n",
    "            tf_subdict[term] = tf_subdict.get(term, 0) + 1\n",
    "\n",
    "        # Update the main term frequency dictionary\n",
    "        for term in tf_subdict:\n",
    "            # If the term not in the term frequency dictionary \n",
    "            if term not in term_freq_dict:\n",
    "                # Create an empty sub-dictionary for storing document numbers and related term frequencies. \n",
    "                term_freq_dict[term] = {}\n",
    "            # The value of the document number sub-dictionary is the term frequency  \n",
    "            term_freq_dict[term][doc_number] = tf_subdict[term]\n",
    "    # Sorted df dictionary in ascending order\n",
    "    term_freq_dict = OrderedDict(sorted(term_freq_dict.items()))\n",
    "    return term_freq_dict\n",
    "\n",
    "# term_frequencies = generate_tf_dictionary(\"trec.sample.xml\", True, True)\n",
    "# # Print the results\n",
    "# for term, doc_freqs in term_frequencies.items():\n",
    "#     print(f\"Term: {term}\")\n",
    "#     for docID, freq in doc_freqs.items():\n",
    "#         print(f\"DocID: {docID}, Term Frequency: {freq}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to calculate DOCUMENT FREQUENCY (DF): The number of documents that a term occurs in the collection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_df_dictionary(xml_file):\n",
    "    # Initialize a dictionary for document frequency of a term \n",
    "    document_freq_dict = {}\n",
    "    \n",
    "    # Call the generate_tf_dictionary to generate a term frequency dictionary\n",
    "    term_freq_dict = generate_tf_dictionary(xml_file, True, True)\n",
    "    \n",
    "    df = 0 # Initilize a value to store the document frequency\n",
    "    \n",
    "    # Iterate over each term (the key) in the term frequency dictionary:\n",
    "    for term in term_freq_dict:\n",
    "        \"\"\" Inside the term frequency dictionary, the value for the \"term\" key is a subdictionary in which \n",
    "         its keys are document numbers that the term occurs and the value is the term frequency in that document numbers\"\"\"\n",
    "        # The df (document frequency) is the number of documents the term occurs in the collection\n",
    "        # The df equals to the length of subdictionary. \n",
    "        df = len(term_freq_dict[term])\n",
    "        \n",
    "        # Assign the document frequency of the term to the document frequency dictionary:\n",
    "        document_freq_dict[term] = df\n",
    "\n",
    "    return document_freq_dict\n",
    "  \n",
    "# doc_freqs = generate_df_dictionary(\"trec.sample.xml\")\n",
    "# for term, df in doc_freqs.items():\n",
    "#     print(f\"term: {term}, df: {df}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a functionn for getting the list of document ID from the collection \n",
    "def all_docIDs(input_XML):\n",
    "    tree = ET.parse(input_XML) \n",
    "    root = tree.getroot()\n",
    "    \n",
    "    docIDs_array = LinkedList()\n",
    "    \n",
    "    # Loop through each <DOC> element\n",
    "    for doc in root.findall(\"DOC\"):\n",
    "        # Find document number element \n",
    "        doc_number = doc.find(\"DOCNO\") \n",
    "        # Add that document number to the all documents IDs array\n",
    "        docIDs_array.add(doc_number.text)\n",
    "        \n",
    "    return docIDs_array\n",
    "\n",
    "# Compute inverse document frequencies (IDF) for each unique term in the collection\n",
    "def compute_idf(df_dict):\n",
    "    # Initialize an empty array for idf value of a term \n",
    "    idf_dict = {}\n",
    "    # Get the total number of document in the collection\n",
    "    total_number_docs = all_docIDs(\"trec.sample.xml\").length()\n",
    "    \n",
    "    # Iterate through the document frequency array \n",
    "    for term in df_dict:\n",
    "        # Get the df\n",
    "        df = df_dict[term]\n",
    "        # Compute idf of each term based on the total number of documents and the df related to the term \n",
    "        idf_dict[term] = math.log(total_number_docs / df, 10)\n",
    "        \n",
    "    return idf_dict\n",
    "\n",
    "# print(all_docIDs(\"trec.sample.xml\").length())\n",
    "# doc_frequency = generate_df_dictionary(\"trec.sample.xml\")\n",
    "# idf = compute_idf(doc_frequency)\n",
    "\n",
    "# for term, idf in idf.items():\n",
    "#     print(f\"term: {term}, idf: {idf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Store the length of each document to use for similarity calculations.  Youâ€™ll also need to store average document length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to store the LENGTH of each document:\n",
    "def compute_document_length(xml_file, stopping, stemming):\n",
    "    \n",
    "    # Initialize an array containing stop words:\n",
    "    with open(\"stoplist\", \"r\") as stoplistFile:\n",
    "        stopwords_array = stoplistFile.read().lower().split() #Tokenization\n",
    "        \n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    tree = ET.parse(xml_file) \n",
    "    root = tree.getroot()\n",
    "    \n",
    "    # Initilizr a dictionary for storing the length of each document\n",
    "    doc_length_dict = {}\n",
    "    \n",
    "    for doc in root.findall(\"DOC\"):\n",
    "        terms_array = []\n",
    "        \n",
    "        # Find the headline and text elements\n",
    "        headline = doc.find(\"HEADLINE\")\n",
    "        text_doc = doc.find(\"TEXT\")\n",
    "        # Find document number element \n",
    "        doc_number = doc.find(\"DOCNO\").text\n",
    "        \n",
    "        # Remove punctuation and apply lower case\n",
    "        punctTable = str.maketrans(string.punctuation, \" \" * len(string.punctuation))\n",
    "        headline = headline.text.lower().translate(punctTable)\n",
    "        text_doc = text_doc.text.lower().translate(punctTable)\n",
    "            \n",
    "        # Get unique terms from headline in each document \n",
    "        for term in headline.split():\n",
    "            if term not in terms_array:\n",
    "                # On stemming \n",
    "                if stopping == True: \n",
    "                    if term not in stopwords_array:\n",
    "                        terms_array.append(term)\n",
    "                # On stemming      \n",
    "                elif stemming == True:\n",
    "                    terms_array.append(stemmer.stem(term)) #Nomalization\n",
    "                # Original \n",
    "                else:\n",
    "                    terms_array.append(term)\n",
    "        \n",
    "        # Get unique terms from text in each document \n",
    "        for term in text_doc.split():\n",
    "            if term not in terms_array:\n",
    "                # On stopping \n",
    "                if stopping == True: \n",
    "                    if term not in stopwords_array:\n",
    "                        terms_array.append(term)\n",
    "                # On stemming        \n",
    "                elif stemming == True:\n",
    "                    term = stemmer.stem(term) #Nomalization\n",
    "                    terms_array.append(stemmer.stem(term)) #Nomalization\n",
    "                # Original     \n",
    "                else:\n",
    "                    terms_array.append(term)\n",
    "                    \n",
    "        doc_length_dict[doc_number] = len(terms_array)\n",
    "        \n",
    "    return doc_length_dict\n",
    "     \n",
    "# doc_length_dict = compute_document_length(\"trec.sample.xml\", True, True)\n",
    "# for doc, length in doc_length_dict.items():\n",
    "#     print(f\"doc: {doc}, D: {length}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2: Define a Similarity Function For tf.idf calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an array containing stop words:\n",
    "with open(\"stoplist\", \"r\") as stoplistFile:\n",
    "    stopwords_array = stoplistFile.read().lower().split() #Tokenization\n",
    "\n",
    "# Define a SIMILARITY FUNCITON \n",
    "def similarity_score(query, term_freq_dict, idf_dict, doc_length_dict, stopping, stemming):\n",
    "    \n",
    "    # Preprocessing the query with either stopping or stemming on:\n",
    "    punctTable = str.maketrans(\"\",\"\",string.punctuation)  # Remove punctuation \n",
    "    \n",
    "    stemmer = PorterStemmer() \n",
    "    \n",
    "    terms = query.strip().translate(punctTable).lower().split() # Preprocessing\n",
    "    \n",
    "    query_array = []\n",
    "    \n",
    "    for term in terms:\n",
    "        if term not in query_array:\n",
    "        # On stopping \n",
    "            if stopping == True: \n",
    "                if term not in stopwords_array:\n",
    "                    query_array.append(term)\n",
    "        # On stemming        \n",
    "            elif stemming == True:\n",
    "                query_array.append(stemmer.stem(term)) #Nomalization\n",
    "        # Original     \n",
    "            else:\n",
    "                query_array.append(term)\n",
    "    \n",
    "    total_doc_length = 0\n",
    "    # Initilize a distionary for similarity scores\n",
    "    tfidf_score = {}\n",
    "    \n",
    "    # Call function to get the totall number of documents in the array \n",
    "    total_number_docs = all_docIDs(\"trec.sample.xml\").length()\n",
    "    \n",
    "    # Iterate through length of each doc in the doc length dictionary \n",
    "    for document in doc_length_dict:\n",
    "        # Sum up the length of all documents \n",
    "        total_doc_length += doc_length_dict[document]\n",
    "    \n",
    "    # Compute average documenent length\n",
    "    avg_doc_length = total_doc_length / total_number_docs\n",
    "    \n",
    "    # Calculate the similarity between the query and the documents\n",
    "    # Iterate each terms in the query:\n",
    "    for term in query_array:\n",
    "        # Iterate document number (the key of the subdictionary inside the tf dictionary) that the term belongs to:\n",
    "        for doc_number in term_freq_dict[term].keys():\n",
    "            # Initalize the tf.idf score for a term in each document. \n",
    "            score = 0 \n",
    "            # Get term frequency of documemnt's term  \n",
    "            tf_document = term_freq_dict[term][doc_number]\n",
    "            \n",
    "            # Get the length of the document \n",
    "            document_length = doc_length_dict[doc_number]\n",
    "            \n",
    "            # Compute tf * idf score without document length normalization \n",
    "            # score = tf_document * idf_dict[term]\n",
    "            \n",
    "            # Compute tf * idf score with document length normalization\n",
    "            score = (tf_document / (tf_document + (2 * document_length / avg_doc_length))) * idf_dict[term]\n",
    "            \n",
    "            # Assign score as the value of the key, document number, in the tfidf_score dictionary\n",
    "            tfidf_score[doc_number] = tfidf_score.get(doc_number,0) + score\n",
    "    \n",
    "    # Sort the dictionary by its values in descending order\n",
    "    tfidf_score = sorted(tfidf_score.items(), key=lambda x:x[1], reverse=True)   \n",
    "        \n",
    "    return tfidf_score\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 4: Run the search three times\n",
    "\n",
    "a)\tno stopping or stemming, save the results in tfidf.results\n",
    "\n",
    "b)\tstemming only, save the results in tfidf.stem.results\n",
    "\n",
    "c)\tstemming and stopping, save the results in tfidf.all.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each query:\n",
    "tfidf_results = {}\n",
    "\n",
    "with open(\"queries.lab3.txt\", 'r') as file:\n",
    "    for line in file:\n",
    "        words = line.strip().lower().split()\n",
    "        query_id = int(words[0])\n",
    "        query_terms = ' '.join(words[1:])\n",
    "        \n",
    "        term_freq_dict = generate_tf_dictionary(\"trec.sample.xml\", True, True)\n",
    "        \n",
    "        doc_freq = generate_df_dictionary(\"trec.sample.xml\")\n",
    "        idf_dict = compute_idf(doc_freq)\n",
    "        \n",
    "        doc_length_dict = compute_document_length(\"trec.sample.xml\", True, True)\n",
    "        \n",
    "        # Calculate the score\n",
    "        score_dict = similarity_score(query_terms, term_freq_dict, idf_dict, doc_length_dict, True, True)\n",
    "        \n",
    "        # Store the results for the query after sorting by score\n",
    "        tfidf_results[query_id] = score_dict\n",
    "\n",
    "    with open(\"tfidf.all.results.txt\", \"w\") as input_file:\n",
    "        # Output the results sorted by query id\n",
    "        for query_id in range(1, 11): \n",
    "            for doc_id, score in tfidf_results.get(query_id, []):\n",
    "                input_file.writelines(f\"{query_id},{doc_id},{score} \\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 5: Write up a lab report that includes the following:\n",
    "\n",
    "a) Details on how you implemented the inverted index in project 2. \n",
    "\n",
    "In project 2, I used the Element Tree (ET) library for parsing the XML file, extracting document numbers, headlines, and text content. And, a data structure, LinkedList, is utilized to keep track of document IDs in which a term occurs. With an application of ElementTree (ET) library, an input XML file containing multiple documents is represented as a tree. The root of the tree is retrieved. Each document is represented by <DOC> element, which has a document ID, headline, and text content. Looping through the file to find all <DOC> element under the root of the XML file; the document ID, headlines, and text are extracted and nomalized (removing punctuation and lowercasing). Stopping and stemming are also considered on or off. Then, a terms_array is initilized to add unique terms. A inverted dictionary named as term_doc_dict dictionary is initialized to contain the unique terms as keys and a LinkedList of document IDs in which that terms occurs in the collection. For each unique term in the terms_array, if it already exists in term_doc_dict dictionary, the document ID is add into the LinkedList document IDs belonging to that term. If a unique term in a document ID does not appear in the term_doc_dict dictionary, a new linkedlist is generated and the document ID in which that unique term occurs is also added into the just created LinkedList document IDs.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "b) Details on how you implemented the search functions (word overlap and Boolean from proj 2), ranked retrieval using the similarity function above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my project 2, I implemented the Boolean retrieval function, using an expression tree. In order to carry out an expression tree from a given query, a TreeNode class is defined, representing each node in a tree and storing data which could be a term or a logical expression later on. In addition, each parent tree node has pointers to its left and right children node. Based on the orignal human readable query, a postfix expression is build. Logical expressions such as \"and\", \"or\", \"not\" are prioritized using a ranking score system (logical_score = {\"not\": 3, \"or\": 2, \"and\": 1}) so that top nodes of building the tree are classified. In this case, \"and\" will be likely to be a parent node than \"not\" and \"or\", and \"or\" tends to be a parent node of \"not\". Looping through each token in the infix expression array, if it is a term, add it to the posfix stack, if ut is a logical operators, add it to the logic stack. When the logical operator is \"not\", it is added to the logic stack. When the logical operators are \"and\", \"or\", while the logic stack is not empty and the top element of the stack is not \"not\", if the top element of the stack has a higher or equal score than the current token, pop the top logical operator from logic stack and appends it to the postfix expression to ensure token with higher score are evaluated first. Then add the current token to the logic stack. And, while there is still logical operators in the logic stack, remove (.pop) it from the stack and add it to the postfix stack. Consequently, from the postfix stack for an expression tree is built in which parent nodes represent logical operators and chil nodes represent terms. \n",
    "\n",
    "For building an expression tree, an empty stack is initialized and logical expressions or logical operators are \"not\", \"or\", \"and\". Each token in the postfix array created above is iterated. If a current token is a term, create a term tree node and push that term into the stack. If a current token is an logical operator, pop 2 values from the stack and make the 2 values become children nodes of the logical operator node and push that current node into the stack. If the logical operator is \"not\", get the top element from the stack, create a \"not\" tree node and then assign the term as the right node of the \"not\" parent node. Otherwise, if token is either \"and\", \"or\", get the 2 top elements from the stack, create a tree node for either \"and\" or \"or\", and then assign the 2 consecutive terms as the right and left children nodes. Afterwards, add that logical operator tree node into the stack. When the first element of the stack is returned, a root node associated with a built logical expression tree is returned. \n",
    "\n",
    "Then 3 boolean search functions such as Boolean_search_or (merging the union of 2 LinkedLists document IDs, so if a doument ID is in either list, it is included in the result posting LinkedList), Boolean_search_and (merging the intersection of 2 LinkedLists document IDs, so only document IDs present in the 2 LinkedLists are included in the result posting LinkedList), and Boolean_search_not (generating a list of document IDs that the given term does not represent by subtracting the LinkedLists document ID of a term from the LinkedList all documents IDs in the collection). Finally, the boolean retrieval function is generated to evaluate the expression tree in which the leaf nodes are terms corresponding to LinkedLists document IDs. Starting from the root, each subtree are recursively evaluated based on the operator at each parent node (and, or, not) and combine their results by calling the 3 defined boolean search functions. \n",
    "\n",
    "For retrieval using the similarity function above, a term frequency dictionary is generated given an XML file in order to calculate the number of times a term occurs in a document. An application of the Element Tree (ET) library is also used for parsing the XML file, looping through each document element in the tree and finding all headline, text elements and document number. The text is also normalized like removing punctuation and lowcasing; stemming and stopping are also applied for later results comparision.  Then, a terms_array is initilized to add unique terms from text and headline parts. Each term frequency is counted in each document and updated in the tf_subdict dictionary. Iterating each term key in the tf_subdict, if the term is not in the general term frequency dictionary (term_freq_dict), an empty sub-dictionary for storing document numbers and related term frequencies is created. And, the term frequency is assigned as the value of the document number key in which the term occurs in the sub-dictionary. The term in the general term frequency dictionary (term_freq_dict) is sorted in ascending order. Generally, the term frequency dictionary (term_freq_dict) has unique terms in the collection as keys and their values are sub-dictionaries in which the keys are documents containing that terms and the values are term frequencies. Secondly, a document frequency dictionary is generated using the term frequency dictionary. It calculates how many documents that the term occurs in a collection (in other words, it's the length of the sub-dictionaries of each terms). \n",
    "\n",
    "Thirdly, with df dictionary and all document IDs dictionary, a function to compute inverse document frequencies (idf) for each unique term in the collection is defined. In the loop of each term in the df dictionary, a document frequency value is retrieved and the log based 10 according to the division between a total number of documents (N) and the df is calculated. Idf is a measure of how much information the word provides whether it's common or rare across a collection of all documents. Fourthly, a compute_document_length function calculates the length of each document in terms of the number of unique terms after preprocessing. Consequently, the similarity_score function takes a query and computes its similarity score with each document using a normalized TF-IDF scores and document normalization. It considers the term frequency, document frequency, and the length of documents to provide a relevance score for each document; stemming and stopping are also considered True or False for results comparision. \n",
    "\n",
    "Finally, the queries were read from the queries.lab3.txt file. Each query is pre-processed similarly to document pre-processing (e.g. if stemming is applied in documents, it is also applied in the query, likewise stopping). And, the similarity scores between the query and the document IDs or document numbers are stored in an dictionary (tfidf_score dictionary). The document normalization considers document length to prevent bias towards longer documents. Documents are scored based on how terms in the query appear in each document, taking into account the tf.idf of terms and the length of documents. The ranked retrieval results are sorted by the similarity score in descending order for each query, which means documents are ranked from most to least relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) A brief commentary on the system as a whole and what you learned from implementing it\n",
    "\n",
    "Word overlap is an approach that a function retrieves all documents in which terms in the query occur. It is an simple algorithm but if it is a common word across a collection such as \"the\", more documents will be retrieved and the results might be potentially irrelevant and lack of precision. So, the downside of the approach might be not actually considering whether a term is common or rare. \n",
    "\n",
    "Boolean Search involves generating an data structure like an expression tree to evaluate logical expression (sentence containing logical operators such as \"and\", \"or\", \"not\") to combine terms and retrieve more relevant documents. It's more precise than the word overlap approach because it retrieves documents that strictly meet the logical criteria. However, for comple queries, it is more difficult to implement an efficient and flexible boolean search and take more time to process. \n",
    "\n",
    "Ranked Retrieval (tf.idf) scores the similarity between a query and documents in a collection. It's based on the frequency of query terms in each document and how common these terms are across all documents. These scores are used to rank documents in terms of its relevance to the query. This approach mainly involves calculating term frequencies, document frequencies, tf.idf scores and document normalization. Ranked Retrieval (tf.idf) also gives a weight to each unique terms through scoring and help us distinguish documents containing common terms versus those containing rare terms. That is aligned with more relevance of retrieved results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) What challenges you faced when implementing it\n",
    "\n",
    "One main challenge I encountered when implementing boolean search is that I was stuck with data structure approach is suitable to use for evaluating the boolean expression. I had a LinkedList document IDs of each term but I was struggling how to retrieve a LinkedList document IDs when there are more than 2 terms and boolean operators (such as \"and\", \"or\", \"not\"). I asked Astrid for help and she recommended me to try to use the expression tree. Expresession tree is similar to a binary tree to some extent. I googled and watched some videos related to how to implement an expression tree. So, given a boolean expression query, \"and\" has a higher rank than \"or\" and \"not\", and \"or\" has a higher rank than \"or\". \"And\" is aimed to be the top of the expression tree. A given infix boolean expression is converted into a postfix using a stack. Then, another stack is used to generate an expression tree from this postfix expression. For each term in a query, a term tree node is created and pushed into the stack. And for each logical operator, 2 terms from the stack are poped and make them become children nodes of the logical operator node then push back that sub-tree onto the stack. The expression tree is executed from the bottom to the top in which the leaves are LinkedList document IDs of terms in the query. From all that knowledge, I proceeded to implement the Boolean search. \n",
    "\n",
    "Resources: https://www.geeksforgeeks.org/evaluation-of-expression-tree/\n",
    "\n",
    "https://www.geeksforgeeks.org/convert-infix-expression-to-postfix-expression/\n",
    "\n",
    "While implementing the similarity scores tf.idf, my challenge was to attempt to understand how to build a term frequency dictionary and how to implement the similarity function, especially with normalization document length. I went to office hours and I understood the concept better. Documents containing query terms have a score of 0 at first, then a similarity score between the query and the document is calculated and added to the documents' scores. A dictionary of documents in which query terms occur and their associated similarity scores with the query are generated. It also took me some time to figure out how to implement stemming and stopping into the code and compare results with my classmates.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Any ideas on how to improve and scale your implementation (e.g. if you were to index a much larger collection, would your implementation run efficiently? If not, how could you improve it?)\n",
    "\n",
    "If I were to index a much larger collection, it would be take a much longer running time for text pre-processing as well as implementing Boolean Search and Ranked Retrieval (tf.idf with document normalization) approaches. As a consequence, my implementation would not run efficiently. To improve it, I could index based on common phrases and rare phrases to optimize quick search and attempt to apply n-grams to improve the relevance and performance of the retrieval system. In addition, I could take into account for partitioning the inverted index across a collection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Compare and contrast the results for the three runs.  For example, what do you notice about them, what do you think explains the differences between them, which ones appear to be the best? \n",
    "\n",
    "In terms of running time, only stemming tooks longer time, around 5 minutes minutes, and a similarity score between a document and a query associated with applying only stemming is smaller compared to without stemming and stopping and with both stemming and stopping. The similarity scores without stopping and stemming is higher than the similarity scores with both stemming and stopping, indicating that stop words are common words in the documents. Applying stemming and stopping results a lower similarity scores but the results might be more relevant. \n",
    "\n",
    "For picking the best model, it depends on the context of the document. If the context contains lots of common words and those words are mentioned in the query, maybe not using stopping could be helpful. In the example 1 as below, not applying stemming and stopping appears to have the highest similarity score between the query and the document. While with both stemming and stopping the similarity score goes down, indicating that reducing words might loose some information although it helps reduce irrelevant common words such as \"the\" and retrieve more relevant documents associated with terms telling the context of the document. With only stemming, the similarity score even goes down, showing that stemming words withough removing stop words is not an efficient way and the short form of the words might not be relavant enough to capture the context of the document.  \n",
    "\n",
    "Example 1: \n",
    "\n",
    "1,3829,1.6252003432981004 (without stemming and stopping)\n",
    "\n",
    "1,3829,1.3768646193254095 (with both stemming and stopping)\n",
    "\n",
    "1,3829,1.3260318143528464 (with only stemming)\n",
    "\n",
    "Example 2:\n",
    "\n",
    "1,3599,1.139406065211134 (without stemming and stopping)\n",
    "\n",
    "1,3599,1.2140747543651442 (with both stemming and stopping)\n",
    "\n",
    "1,3599,1.1427578663149143 (only stemming)\n",
    "\n",
    "However, things are different in the second example. The combined approach of stemming and stopping seems to be the most effective for this document, indicating that focusing on terms representing meaning of a document works effeciently. Only stemming brings an slightly improvement compared to not applying stemming and stopping, showing that removing stopwords might help improve the similarity score and even retrieve top relevant documents. In general, the second example illustrates the benefits of stemming and stopping over retaining the original text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g) Describe your approach for evaluating and comparing the results.\n",
    "\n",
    "To compare the results, I created 3 txt files with tf.idf.results (no stopping and stemming), tfidf.stem.results (with stemming only), and tfidf.all.results (stemming and stopping). In the tf.idf.results, the stopping and stemming arguments are False, False, respectively. And, in the tfidf.stem.results, the stopping and stemming arguments are False, True, respectively. Similarly, in the tf.idf.results, the stopping and stemming arguments are True, True, respectively.\n",
    "\n",
    "The query is preprocessed similarly to the document. Stemming applied in the documents is also used in the query. Likewise, stopping applied in the documents is also applied in the query. That ensures the terms in the query and documets are consistent. Different scores getting from the 3 approaches are written in the 3 abovementioned txt files and I used the search function to find a specific document and then compare their 3 results of similar scores manually. My approach for evaluating and comparing the results is not effcient."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
